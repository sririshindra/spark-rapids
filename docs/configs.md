<!-- Generated by RapidsConf.help. DO NOT EDIT! -->
# Rapids Plugin 4 Spark Configuration
The following is the list of options that `rapids-plugin-4-spark` supports.

On startup use: `--conf [conf key]=[conf value]`. For example:

```
${SPARK_HOME}/bin/spark --jars 'rapids-4-spark-0.1-SNAPSHOT.jar,cudf-0.14-SNAPSHOT-cuda10.jar' \
--conf spark.plugins=ai.rapids.spark.SQLPlugin \
--conf spark.rapids.sql.incompatibleOps.enabled=true
```

At runtime use: `spark.conf.set("[conf key]", [conf value])`. For example:

```
scala> spark.conf.set("spark.rapids.sql.incompatibleOps.enabled", true)
```

## General Configuration
Name | Description | Default Value
-----|-------------|--------------
spark.rapids.memory.gpu.allocFraction|The fraction of total GPU memory that should be initially allocated for pooled memory. Extra memory will be allocated as needed, but it may result in more fragmentation.|0.9
spark.rapids.memory.gpu.asyncSpillFraction|When reported GPU memory usage divided by total GPU memory is above this ratio, cached data will spill asynchronously.|0.75
spark.rapids.memory.gpu.debug.enabled|If memory management is enabled and this is true GPU memory allocations are tracked and printed out when the process exits. This should not be used in production.|false
spark.rapids.memory.gpu.pooling.enabled|Should RMM act as a pooling allocator for GPU memory, or should it just pass through to CUDA memory allocation directly.|true
spark.rapids.memory.gpu.spillFraction|When reported GPU memory usage divided by total GPU memory is above this ratio, the allocator thread will block until spilling of cached data has completed.|0.85
spark.rapids.memory.pinnedPool.size|The size of the pinned memory pool in bytes unless otherwise specified. Use 0 to disable the pool.|0
spark.rapids.memory.uvm.enabled|UVM or universal memory can allow main host memory to act essentially as swap for device(GPU) memory. This allows the GPU to process more data than fits in memory, but can result in slower processing. This is an experimental feature.|false
spark.rapids.shuffle.ucx.enabled|When set to true, enable the UCX transfer method for shuffle files.|false
spark.rapids.shuffle.ucx.fetch.wait.period|Maximum value of the Initial time to waitin sending fetch requests|50
spark.rapids.shuffle.ucx.handleLocalShuffle|When set to true, allow UCX to transfer host-local shuffle files.|true
spark.rapids.shuffle.ucx.handleRemoteShuffle|When set to true, allow UCX to transfer remote shuffle files.|false
spark.rapids.shuffle.ucx.managementServerHost|The host to be used to start the management server|null
spark.rapids.shuffle.ucx.receive.async|Decides if fetches should be async|false
spark.rapids.shuffle.ucx.throttle.enabled|When set to true, enable the UCX throttle on send.|false
spark.rapids.shuffle.ucx.useWakeup|When set to true, use UCX's event-based progress (epoll) in order to wake up the progress thread when needed, instead of a hot loop.|false
spark.rapids.shuffle.ucx.wait.period|Initial time to wait in ms for requests that cannot be processed by the shuffle manager|5
spark.rapids.shuffle.ucx.wait.period.update.enabled|Decides if wait period should be updates based on transaction stats|false
spark.rapids.sql.batchSizeRows|Set the target number of rows for a GPU batch. Splits sizes for input data is covered by separate configs.|1000000
spark.rapids.sql.concurrentGpuTasks|Set the number of tasks that can execute concurrently per GPU. Tasks may temporarily block when the number of concurrent tasks in the executor exceeds this amount. Allowing too many concurrent tasks on the same GPU may lead to GPU out of memory errors.|1
spark.rapids.sql.enabled|Enable (true) or disable (false) sql operations on the GPU|true
spark.rapids.sql.explain|Explain why some parts of a query were not placed on a GPU or not. Possible values are ALL: print everything, NONE: print nothing, NOT_ON_GPU: print only did not go on the GPU|NONE
spark.rapids.sql.hasNans|Config to indicate if your data has NaN's. Cudf doesn't currently support NaN's properly so you can get corrupt data if you have NaN's in your data and it runs on the GPU.|true
spark.rapids.sql.hashOptimizeSort.enabled|Whether sorts should be inserted after some hashed operations to improve output ordering. This can improve output file sizes when saving to columnar formats.|false
spark.rapids.sql.incompatibleOps.enabled|For operations that work, but are not 100% compatible with the Spark equivalent set if they should be enabled by default or disabled by default.|false
spark.rapids.sql.reader.batchSizeBytes|Soft limit on the maximum number of bytes the reader reads per batch. The readers will read chunks of data until this limit is met or exceeded. Note that the csv and parquet readers use this limit even if the underlying file is compressed.|2147483647
spark.rapids.sql.reader.batchSizeCompressedBytes|Soft limit on the maximum number of bytes the reader reads per batch when reading compressed files. The readers will read chunks of compressed data until this limit is met or exceeded.|1073741823
spark.rapids.sql.reader.batchSizeRows|Soft limit on the maximum number of rows the reader will read per batch. The orc and parquet readers will read row groups until this limit is met or exceeded. The limit is respected by the csv reader.|2147483647
spark.rapids.sql.replaceSortMergeJoin.enabled|Allow replacing sortMergeJoin with HashJoin|true
spark.rapids.sql.shuffle.spillThreads|Number of threads used to spill shuffle data to disk in the background.|6
spark.rapids.sql.variableFloatAgg.enabled|Spark assumes that all operations produce the exact same result each time. This is not true for some floating point aggregations, which can produce slightly different results on the GPU as the aggregation is done in parallel.  This can enable those operations if you know the query is only computing it once.|false

## Fine Tuning
_Rapids Plugin 4 Spark_ can be further configured to enable or disable specific
expressions and to control what parts of the query execute using the GPU or
the CPU.

Please leverage the `spark.rapids.sql.explain` setting to get feedback from the
plugin as to why parts of a query may not be executing on the GPU.

**NOTE:** Setting `spark.rapids.sql.incompatibleOps.enabled=true` will enable all
the settings in the table below which are not enabled by default due to
incompatibilities.

### Expressions
Name | Description | Default Value | Incompatibilities
-----|-------------|---------------|------------------
spark.rapids.sql.expression.Abs|absolute value|true|None|
spark.rapids.sql.expression.Acos|inverse cosine|true|None|
spark.rapids.sql.expression.Acosh|inverse hyperbolic cosine|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Add|addition|true|None|
spark.rapids.sql.expression.Alias|gives a column a name|true|None|
spark.rapids.sql.expression.And|logical and|true|None|
spark.rapids.sql.expression.Asin|inverse sine|true|None|
spark.rapids.sql.expression.Asinh|inverse hyperbolic sine|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.AtLeastNNonNulls|checks if number of non null/Nan values is greater than a given value|true|None|
spark.rapids.sql.expression.Atan|inverse tangent|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Atanh|inverse hyperbolic tangent|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.AttributeReference|references an input column|true|None|
spark.rapids.sql.expression.BitwiseAnd|Returns the bitwise AND of the operands|true|None|
spark.rapids.sql.expression.BitwiseNot|Returns the bitwise NOT of the operands|true|None|
spark.rapids.sql.expression.BitwiseOr|Returns the bitwise OR of the operands|true|None|
spark.rapids.sql.expression.BitwiseXor|Returns the bitwise XOR of the operands|true|None|
spark.rapids.sql.expression.CaseWhen|CASE WHEN expression|true|None|
spark.rapids.sql.expression.Cast|convert a column of one type of data into another type|true|None|
spark.rapids.sql.expression.Cbrt|cube root|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Ceil|ceiling of a number|true|None|
spark.rapids.sql.expression.Coalesce|Returns the first non-null argument if exists. Otherwise, null.|true|None|
spark.rapids.sql.expression.Contains|Contains|true|None|
spark.rapids.sql.expression.Cos|cosine|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Cosh|hyperbolic cosine|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.DateDiff|datediff|true|None|
spark.rapids.sql.expression.DayOfMonth|get the day of the month from a date or timestamp|true|None|
spark.rapids.sql.expression.Divide|division|true|None|
spark.rapids.sql.expression.EndsWith|Ends With|true|None|
spark.rapids.sql.expression.EqualTo|check if the values are equal|true|None|
spark.rapids.sql.expression.Exp|Euler's number e raised to a power|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Expm1|Euler's number e raised to a power minus 1|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Floor|floor of a number|true|None|
spark.rapids.sql.expression.FromUnixTime|get the String from a unix timestamp|true|None|
spark.rapids.sql.expression.GreaterThan|> operator|true|None|
spark.rapids.sql.expression.GreaterThanOrEqual|>= operator|true|None|
spark.rapids.sql.expression.If|IF expression|true|None|
spark.rapids.sql.expression.In|IN operator|true|None|
spark.rapids.sql.expression.InSet|INSET operator|true|None|
spark.rapids.sql.expression.IntegralDivide|division with a integer result|true|None|
spark.rapids.sql.expression.IsNaN|checks if a value is NaN|true|None|
spark.rapids.sql.expression.IsNotNull|checks if a value is not null|true|None|
spark.rapids.sql.expression.IsNull|checks if a value is null|true|None|
spark.rapids.sql.expression.KnownFloatingPointNormalized|tag to prevent redundant normalization|false|This is not 100% compatible with the Spark version because when enabling these, there may be extra groups produced for floating point grouping keys (e.g. -0.0, and 0.0)|
spark.rapids.sql.expression.LessThan|< operator|true|None|
spark.rapids.sql.expression.LessThanOrEqual|<= operator|true|None|
spark.rapids.sql.expression.Literal|holds a static value from the query|true|None|
spark.rapids.sql.expression.Log|natural log|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Lower|String lowercase operator|false|This is not 100% compatible with the Spark version because in some cases unicode characters change byte width when changing the case. The GPU string conversion does not support these characters. For a full list of unsupported characters see https://github.com/rapidsai/cudf/issues/3132|
spark.rapids.sql.expression.Month|get the month from a date or timestamp|true|None|
spark.rapids.sql.expression.Multiply|multiplication|true|None|
spark.rapids.sql.expression.NaNvl|evaluates to `left` iff left is not NaN, `right` otherwise.|true|None|
spark.rapids.sql.expression.Not|boolean not operator|true|None|
spark.rapids.sql.expression.Or|logical or|true|None|
spark.rapids.sql.expression.Pow|lhs ^ rhs|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Rand|Generate a random column with i.i.d. uniformly distributed values in [0, 1)|true|None|
spark.rapids.sql.expression.Remainder|remainder or modulo|true|None|
spark.rapids.sql.expression.Rint|Rounds up a double value to the nearest double equal to an integer|true|None|
spark.rapids.sql.expression.ShiftLeft|Bitwise shift left (<<)|true|None|
spark.rapids.sql.expression.ShiftRight|Bitwise shift right (>>)|true|None|
spark.rapids.sql.expression.ShiftRightUnsigned|Bitwise unsigned shift right (>>>)|true|None|
spark.rapids.sql.expression.Sin|sine|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Sinh|hyperbolic sine|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.SortOrder|sort order|true|None|
spark.rapids.sql.expression.Sqrt|square root|true|None|
spark.rapids.sql.expression.StartsWith|Starts With|true|None|
spark.rapids.sql.expression.StringLocate|Substring search operator|true|None|
spark.rapids.sql.expression.Substring|Substring operator|true|None|
spark.rapids.sql.expression.Subtract|subtraction|true|None|
spark.rapids.sql.expression.Tan|tangent|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.Tanh|hyperbolic tangent|false|This is not 100% compatible with the Spark version because floating point results in some cases may differ with the JVM version by a small amount|
spark.rapids.sql.expression.UnaryMinus|negate a numeric value|true|None|
spark.rapids.sql.expression.UnaryPositive|a numeric value with a + in front of it|true|None|
spark.rapids.sql.expression.Upper|String uppercase operator|false|This is not 100% compatible with the Spark version because in some cases unicode characters change byte width when changing the case. The GPU string conversion does not support these characters. For a full list of unsupported characters see https://github.com/rapidsai/cudf/issues/3132|
spark.rapids.sql.expression.Year|get the year from a date or timestamp|true|None|
spark.rapids.sql.expression.AggregateExpression|aggregate expression|true|None|
spark.rapids.sql.expression.Average|average aggregate operator|true|None|
spark.rapids.sql.expression.Count|count aggregate operator|true|None|
spark.rapids.sql.expression.First|first aggregate operator|true|None|
spark.rapids.sql.expression.Last|last aggregate operator|true|None|
spark.rapids.sql.expression.Max|max aggregate operator|true|None|
spark.rapids.sql.expression.Min|min aggregate operator|true|None|
spark.rapids.sql.expression.Sum|sum aggregate operator|true|None|
spark.rapids.sql.expression.NormalizeNaNAndZero|normalize nan and zero|false|This is not 100% compatible with the Spark version because when enabling these, there may be extra groups produced for floating point grouping keys (e.g. -0.0, and 0.0)|

### Execution
Name | Description | Default Value | Incompatibilities
-----|-------------|---------------|------------------
spark.rapids.sql.exec.CoalesceExec|The backend for the dataframe coalesce method|true|None|
spark.rapids.sql.exec.FileSourceScanExec|Reading data from files, often from Hive tables|true|None|
spark.rapids.sql.exec.FilterExec|The backend for most filter statements|true|None|
spark.rapids.sql.exec.GenerateExec|The backend for operations that generate more output rows than input rows like explode.|true|None|
spark.rapids.sql.exec.GlobalLimitExec|Limiting of results across partitions|true|None|
spark.rapids.sql.exec.LocalLimitExec|Per-partition limiting of results|true|None|
spark.rapids.sql.exec.ProjectExec|The backend for most select, withColumn and dropColumn statements|true|None|
spark.rapids.sql.exec.SortExec|The backend for the sort operator|true|None|
spark.rapids.sql.exec.UnionExec|The backend for the union operator|true|None|
spark.rapids.sql.exec.HashAggregateExec|The backend for hash based aggregations|true|None|
spark.rapids.sql.exec.DataWritingCommandExec|Writing data|true|None|
spark.rapids.sql.exec.BatchScanExec|The backend for most file input|true|None|
spark.rapids.sql.exec.BroadcastExchangeExec|The backend for broadcast exchange of data|true|None|
spark.rapids.sql.exec.ShuffleExchangeExec|The backend for most data being exchanged between processes|true|None|
spark.rapids.sql.exec.BroadcastHashJoinExec|Implementation of join using broadcast data|true|None|
spark.rapids.sql.exec.ShuffledHashJoinExec|Implementation of join using hashed shuffled data|true|None|
spark.rapids.sql.exec.SortMergeJoinExec|Sort merge join, replacing with shuffled hash join|true|None|

### Scans
Name | Description | Default Value | Incompatibilities
-----|-------------|---------------|------------------
spark.rapids.sql.input.CSVScan|CSV parsing|true|None|
spark.rapids.sql.input.OrcScan|ORC parsing|true|None|
spark.rapids.sql.input.ParquetScan|Parquet parsing|true|None|

### Partitioning
Name | Description | Default Value | Incompatibilities
-----|-------------|---------------|------------------
spark.rapids.sql.partitioning.HashPartitioning|Hash based partitioning|true|None|
spark.rapids.sql.partitioning.RangePartitioning|Range Partitioning|true|None|
spark.rapids.sql.partitioning.RoundRobinPartitioning|Round Robin Partitioning|true|None|
spark.rapids.sql.partitioning.SinglePartition$|Single Partitioning|true|None|
